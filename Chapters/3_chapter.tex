\chapterimage{head2.png} % Chapter heading image
\chapter{\normalsize Lecture 3: Transformer Decoders, LLMs and GPTs}


\subsubsection*{Transformer Architecture Overview}
\begin{itemize}
    \item \textbf{Transformer Decoder:} The component responsible for generating output sequences token by token
    \item \textbf{Essential Components:}
    \begin{enumerate}
        \item Masked Multi-Head Attention (enforces autoregressive property)
        \item Multi-Head Cross-Attention (connects to encoder if present)
        \item Feed-Forward Networks
        \item Layer Normalization
        \item Residual connections
    \end{enumerate}
\end{itemize}

\subsubsection*{Key Transformer Model Types}
\begin{itemize}
    \item \textbf{Encoder-Decoder Models:} Translation, summarization (original Transformer)
    \item \textbf{Encoder-Only Models:} BERT, text classification, understanding
    \item \textbf{Decoder-Only Models:} GPT family, text generation, most modern LLMs
\end{itemize}

\subsubsection*{Attention Mechanisms}
\begin{itemize}
    \item \textbf{Self-Attention:} Token relationships within the same sequence
    \item \textbf{Masked Self-Attention:} Only allows a token to attend to previous tokens (slides 24--25)
    \item \textbf{Cross-Attention:} Allows decoder to attend to encoder outputs (slide 26)
    \item \textbf{Multi-Head Attention:} Allows multiple attention patterns in parallel
\end{itemize}

\subsubsection*{Autoregressive Generation (Critical for GPT-style models)}
\begin{itemize}
    \item Tokens generated sequentially, with each new token conditioned on all previous tokens
    \item \textbf{Formula:}
    \[
    P(w_1, w_2, \dots, w_n) = \prod_{i} P(w_i \mid w_1, w_2, \dots, w_{i-1})
    \]
    \item Enforced via \textbf{causal masking} in the attention mechanism
\end{itemize}
\section{GPT-Style Models vs BERT}

\subsection*{GPT Architecture \& Function}
\begin{itemize}
    \item \textbf{Decoder-only} transformer model
    \item \textbf{Unidirectional} (left-to-right) attention
    \item \textbf{Autoregressive} next token prediction
    \item Trained using standard language modeling objective
    \item Generate text by sampling from conditional probability distributions
\end{itemize}

\subsection*{BERT vs. GPT Comparison (Slide 32--33)}
\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Feature} & \textbf{BERT} & \textbf{GPT} \\
\hline
Model Type & Encoder-Only & Decoder-Only \\
\hline
Direction & Bidirectional & Unidirectional (left-to-right) \\
\hline
Pre-training & Masked Language Modeling (MLM) & Autoregressive Language Modeling \\
\hline
Fine-tuning & Task-specific layer added on top & Few-shot prompting or fine-tuning \\
\hline
Use Case & Understanding (classification, NER) & Generation (text completion, creative writing) \\
\hline
Original Creator & Google AI & OpenAI \\
\hline
\end{tabular}
\end{center}

\subsection*{Training Objectives}
\begin{itemize}
    \item \textbf{GPT:} Predict next token given previous tokens
    \item \textbf{BERT:} Predict masked tokens using bidirectional context
\end{itemize}
\section{Machine Translation with Transformers}

\subsection*{Encoder-Decoder for Translation}
\begin{itemize}
    \item \textbf{Input:} Source language sequence processed by encoder
    \item \textbf{Output:} Target language sequence generated by decoder
    \item \textbf{Cross-attention:} Decoder queries attend to encoder key-value pairs
    \item \textbf{Training objective:} Teacher forcing with target sequence
\end{itemize}

\subsection*{Translation Process (From Slides 12--23)}
\begin{enumerate}
    \item Source sentence tokenized and fed to encoder
    \item Encoder builds contextual representations
    \item Decoder starts with special start token
    \item Decoder generates target tokens one-by-one
    \item Cross-attention allows decoder to focus on relevant source tokens
    \item Process continues until end token is generated
\end{enumerate}

\subsection*{Difference from Decoder-Only Models}
\begin{itemize}
    \item Encoder-decoder has explicit source encoding step
    \item Cross-attention mechanism connects source and target
    \item More parameter-efficient for translation tasks
    \item Better handling of source language nuances
\end{itemize}
\section{Key Diagrams \& Workflows}

\subsection*{Transformer Decoder Structure (Slide 24)}
The presentation shows the decoder with:
\begin{itemize}
    \item Masked Multi-Head Attention block
    \item Multi-Head Cross-Attention block (for encoder-decoder models)
    \item Feed-Forward Networks
    \item Layer normalization and residual connections
\end{itemize}

\subsection*{Masked Attention Mechanism (Slide 25)}
\begin{itemize}
    \item Shows how attention is masked to prevent looking at future tokens
    \item Contrast between standard self-attention and masked self-attention
    \item Implementation by setting future position scores to negative infinity
\end{itemize}

\subsection*{Autoregressive Decoder Workflow (Slides 17--22)}
\begin{itemize}
    \item Step-by-step illustration of how tokens are generated
    \item Starts with special token \texttt{<START>}
    \item Generates one token at a time
    \item Each new token depends on all previous tokens
\end{itemize}

\subsection*{Decoding Strategies (Slides 40--47)}
\begin{itemize}
    \item \textbf{Greedy decoding:} Always select highest probability token
    \item \textbf{Beam search:} Maintain multiple hypotheses
    \item \textbf{Sampling:} Draw from probability distribution (with temperature control)
\end{itemize}
\section{Practice Quiz Questions}

\begin{enumerate}
    \item \textbf{Q:} What is the key difference between the attention mechanism in BERT and GPT?\\
    \textbf{A:} BERT uses bidirectional self-attention, while GPT uses masked (causal) self-attention that only allows a token to attend to previous tokens.

    \item \textbf{Q:} Why can't GPT process all output tokens in parallel during inference?\\
    \textbf{A:} Due to the autoregressive nature, each token depends on previously generated tokens, forcing sequential generation.

    \item \textbf{Q:} What is the purpose of cross-attention in an encoder-decoder transformer?\\
    \textbf{A:} Cross-attention allows the decoder to attend to relevant parts of the encoder's output, connecting source and target sequences.

    \item \textbf{Q:} How does beam search differ from greedy decoding?\\
    \textbf{A:} Greedy decoding selects the most probable token at each step, while beam search maintains multiple hypotheses and selects the overall most probable sequence.

    \item \textbf{Q:} Why might multinomial sampling with temperature control produce more diverse text than beam search?\\
    \textbf{A:} Sampling introduces randomness that helps avoid repetitive patterns, while temperature control allows balancing between diversity and coherence.

    \item \textbf{Q:} What problem in tokenization does Byte Pair Encoding (BPE) address?\\
    \textbf{A:} BPE balances vocabulary size and sequence length by using subword units, handling rare words better than word-level tokenization while being more efficient than character-level tokenization.
\end{enumerate}
\section{Common Pitfalls \& Advanced Questions}

\subsection*{Causal Masking in GPT}
\begin{itemize}
    \item \textbf{Mechanism:} Sets attention scores for future positions to negative infinity
    \item \textbf{Purpose:} Enforces autoregressive constraint
    \item \textbf{Pitfall:} Confusing with padding masks (different purpose)
    \item \textbf{Examiner might ask:} How does causal masking contribute to the training efficiency of GPT models?
\end{itemize}

\subsection*{Parallel Processing Limitations}
\begin{itemize}
    \item \textbf{Training:} Can be parallelized because ground truth is available
    \item \textbf{Inference:} Must be sequential due to autoregressive nature
    \item \textbf{Pitfall:} Assuming inference can be parallelized like training
    \item \textbf{Examiner might ask:} What approaches might improve inference speed in autoregressive models?
\end{itemize}

\subsection*{Long-Range Dependencies}
\begin{itemize}
    \item \textbf{Challenge:} Attention complexity grows quadratically with sequence length
    \item \textbf{Solutions:} Sparse attention, sliding windows, hierarchical attention
    \item \textbf{Pitfall:} Not understanding the computational bottlenecks
    \item \textbf{Examiner might ask:} How do modern LLMs handle context windows of 100K+ tokens?
\end{itemize}

\subsection*{Tokenization Tradeoffs}
\begin{itemize}
    \item \textbf{Character-level:} Small vocabulary, long sequences
    \item \textbf{Word-level:} Large vocabulary, OOV problems
    \item \textbf{Subword (BPE):} Compromise, $\sim$4 characters per token in English
    \item \textbf{Examiner might ask:} How might tokenization biases affect model performance across languages?
\end{itemize}

\subsection*{Decoding Strategy Selection}
\begin{itemize}
    \item \textbf{When to use beam search:} When the most probable overall sequence is desired
    \item \textbf{When to use sampling:} When diversity is important
    \item \textbf{Pitfall:} Using the wrong strategy for the application
    \item \textbf{Examiner might ask:} Why does beam search tend to produce repetitive text in open-ended generation?
\end{itemize}

\subsection*{Evaluation Challenges}
\begin{itemize}
    \item \textbf{Automated metrics:} BLEU, ROUGE have limitations
    \item \textbf{Human evaluation:} Subjective but often necessary
    \item \textbf{Pitfall:} Over-reliance on automated metrics
    \item \textbf{Examiner might ask:} How would you evaluate an LLM for factual accuracy versus creativity?
\end{itemize}
